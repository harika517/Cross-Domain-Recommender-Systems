{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "import pyspark.sql\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as func\n",
    "import string\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session configuration\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Book Recommender\") \\\n",
    "    .config(\"spark.recommender\", \"1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Book Recommender\") \\\n",
    "                  .set(\"spark.executor.heartbeatInterval\", \"200000\") \\\n",
    "                  .set(\"spark.network.timeout\", \"300000\") # Increasing the network timeout \n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the book dataset which is been preprocessed and converted to .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data with userid, title, ratings\n",
    "ratings_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('user_book_rating.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data with 500 unique users for testing\n",
    "unique_user_data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('unique_user_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user_id in the dataset is in base 16, but the ALS train function need the user_id in int base 10. Therefore converting user ids to have a unique int base 10 number using the StringIndexer() function and added as the new column 'int_user_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol=\"user_id\", outputCol=\"int_user_id\")\n",
    "model = stringIndexer.fit(ratings_df)\n",
    "ratings_df = model.transform(ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+-----------+--------------------+\n",
      "|             user_id| book_id|rating|int_user_id|               title|\n",
      "+--------------------+--------+------+-----------+--------------------+\n",
      "|e258cbe5ccb791a14...|  789813|     3|     290145|No Colder Place (...|\n",
      "|c6d23cb41fdcd4a5d...|19291332|     4|     254922|One in a Million ...|\n",
      "|ba915128f7275e70f...|  261122|     5|     239184|The Dead Girls' D...|\n",
      "|8bfbf1b344efdb2ca...| 9361589|     3|     179483|    The Night Circus|\n",
      "|f15b6dcc2311ab919...|  234225|     4|     309302|Dune (Dune Chroni...|\n",
      "+--------------------+--------+------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|             user_id|int_user_id|\n",
      "+--------------------+-----------+\n",
      "|a6dcd679dca6344a0...|     213915|\n",
      "|a16922af070d21d41...|     207010|\n",
      "|c9583a77ed90d0461...|     258083|\n",
      "|a9356165bd3332802...|     216948|\n",
      "|632432e4fba80ee05...|     127095|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Created a dataframe having about 500 users to get their recommendations\n",
    "unique_user_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|int_user_id| book_id|rating|\n",
      "+-----------+--------+------+\n",
      "|     290145|  789813|     3|\n",
      "|     254922|19291332|     4|\n",
      "|     239184|  261122|     5|\n",
      "|     179483| 9361589|     3|\n",
      "|     309302|  234225|     4|\n",
      "+-----------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Forming a dataframe with only the required columns\n",
    "required_df = ratings_df[['int_user_id', 'book_id', 'rating']]\n",
    "required_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7567357\n"
     ]
    }
   ],
   "source": [
    "# Number of records in the dataset\n",
    "\n",
    "required_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data for training, testing and validation\n",
    "\n",
    "(trainingData,validationData,testData) = required_df.randomSplit([0.6,0.2,0.2]) \n",
    "\n",
    "validation_for_predict = validationData.select('int_user_id','book_id')\n",
    "test_for_predict = testData.select('int_user_id','book_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+\n",
      "|int_user_id| book_id|rating|\n",
      "+-----------+--------+------+\n",
      "|         19|33954483|     4|\n",
      "|         51|20821149|     5|\n",
      "|        117|25095546|     5|\n",
      "|        176|26244548|     4|\n",
      "|        201|   47683|     4|\n",
      "+-----------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters obtained after several trials\n",
    "seed = 15 # For initial matrix factorization model\n",
    "iterations = 10 # after trying 5, 15, 20\n",
    "reg = 0.09 #upon trails found this values as optimum for regularization\n",
    "rank = 20 # [4, 8, 12, 20] number of factors to choose for matrix factorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converting the dataframe to RDD using df.RDD\n",
    "\n",
    "model = ALS.train(trainingData, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=reg)\n",
    "    \n",
    "predictions = model.predictAll(validation_for_predict.rdd).map(lambda r: ((r[3], r[1]), r[2]))\n",
    "ates_and_preds = validationData.rdd.map(lambda r: ((str(r[0]), int(r[1])), float(r[2]))).join(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((158480, 15808474), 0.0),\n",
       " ((158480, 547448), 0.0),\n",
       " ((158480, 23512999), 0.0),\n",
       " ((146272, 10429092), 0.17743873563377147),\n",
       " ((24360, 410615), -1.0944712442902218)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Predicting rating of all books for the users of test dataset\n",
    "predictions_test = model.predictAll(test_for_predict.rdd).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "predictions_test.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rating Prediction**\n",
    "\n",
    "\n",
    "Training the model with the whole data, and getting the top five movie recmmendatons for randomly chosen 500 users. The output is saved in the dictionary with user_id as the key and the recommendations with predicted ratings as the value. This dictionary is pickled and used further in the cross domain recommender to show top five books and movies recommended to the chosen / test user_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecommendations(user,testDf,trainDf,model):\n",
    "    \n",
    "    userDf = testDf.filter(testDf.int_user_id == user)\n",
    "    \n",
    "    # Passing only the unread books, by subtracting the read books of the user\n",
    "    mov = trainDf.select('book_id').subtract(userDf.select('book_id'))\n",
    "    \n",
    "    # covert our dataframe into RDD as predicrtAll() takes only RDD format for arguments\n",
    "    pred_rat = model.predictAll(mov.rdd.map(lambda x: (user, x[0]))).collect()\n",
    "    \n",
    "    # Get the top five recommendations\n",
    "    recommendations = sorted(pred_rat, key=lambda x: x[2], reverse=True)[:5]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "#Creating a dictionary to hold the user_id as key and recommendations as value\n",
    "output_dict = {}\n",
    "\n",
    "# Call getRecommendations method\n",
    "for user in unique_user_data.select('int_user_id').collect():\n",
    "    \n",
    "    \n",
    "    derived_rec = getRecommendations(user[0],testData,trainingData,model)\n",
    "    \n",
    "    user_name = unique_user_data.filter(unique_user_data.int_user_id==user[0]).select('user_id')\n",
    "    output_dict[user_name.collect()[0]['user_id']] = []\n",
    "    #print(user_name.collect())\n",
    "    for i in range(len(derived_rec)):\n",
    "        \n",
    "        # to get the title of the book from the book_id\n",
    "        result = ratings_df.filter(ratings_df.book_id==derived_rec[i][1]).select('title')\n",
    "        # appending the recommendations to the dictionary value\n",
    "        output_dict[user_name.collect()[0]['user_id']].append((result.rdd.flatMap(list).first(), derived_rec[i][2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dictionary is searched by key = user_id of interest, the recommended movies and the respective predicted ratings are shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Queen of Shadows (Throne of Glass, #4)', 4.950911418089894), ('Wallbanger (Cocktail, #1)', 4.185194464252074), ('Harry Potter and the Prisoner of Azkaban (Harry Potter, #3)', 4.1137276488376795), ('Tough Luck (Hard Rock Roots, #3)', 3.8615057209867754), ('Roman Crazy', 3.808723370533395)]\n"
     ]
    }
   ],
   "source": [
    "# Getting book recommendations for the test user\n",
    "id_ = '8438f7cb6879b1e684d944e8afd50ec1'\n",
    "val = output_dict.get(id_) # key for the dictionary is the user_id\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the output dictionary to json file\n",
    "with open('book_rec.txt', 'w') as file:\n",
    "    file.write(json.dumps(output_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the result of recommendations, to show further in the cross domain recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Pickling the dictionery to use the values further\n",
    "with open('book_rec.pickle', 'wb') as handle:\n",
    "    pickle.dump(output_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('book_rec.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print (output_dict == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html <br>\n",
    "https://sjsu.instructure.com/files/57380718/download?download_frd=1 <br>\n",
    "https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
